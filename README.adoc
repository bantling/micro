// SPDX-License-Identifier: Apache-2.0
:doctype:article

= Micro

Micro is a small library of useful stuff for small projects, particularly (but not limited to) REST/database projects.

== Packages

* constraint
** defines generic type constraints, some are similar to golang.org/x/exp/constraints
* conv
** converts between numeric types, panicking if any loss of precision would occur
* funcs
** slices:
*** access elements safely
*** flatten multiple dimensions into one
*** reverse elements
*** sort elements
** maps: access keys safely
** filter - func(T) bool:
*** compose with and, or, not
*** generate filters for comparisons (<, <=, ==, >=, >)
*** generate filters for is negative, is non-negative, is positive, is nil, is non nil
** compose any number of funcs that accept and receive same type
** compose up to 10 funcs that accept and return different types
** ternary - take a bool, true value or supplier, false value or supplier, and return true or false value
** min/max
** nil handling
** supplier generators
** generate a func that ignores result of another func
** TryTo is a replacement for hard to write idiomatic go code that acts like a Java try/catch/finally block:
   Accepts a func for a try block, a func for a catch block (only invoked if try block panics), and any number of
   closer funcs that close resources regardless of whether the try func panics.
* iter
** defines Iter type that can iterate anything
** based on iterating funcs, a func of no args that returns (value, bool), where the value is only relevant if the bool
   is true
** A number of constructors are provided for hard-coded values, slices, maps, io.reader, concat multiple iters
** Next, value, unread, and Must funcs to retrieve values
** Provides streaming functionality (similar to that of Java 8 streams).
    Most functions accept ands return an Iter, providing some kind of transform.
*** funcs for first, map, filter, reduce, expand, skip, limit, peek, generate infinite series, all match, any match,
   none match, count, distinct, duplicate, reverse, sort, and parallel
*** control over how and when parallel execution occurs, default algorithm provided
* json
** Value type that describes any kind of JSON value
** Convert between go types to Value and vice-versa (eg, map[string]any -> Value of type Object -> map[string]any)
** default numeric type is NumberString, but custom conversion functions can be used for some other numeric type
* util
** Provides ErrorReader to return a specified error after reading a specified set of bytes; useful for unit tests.

== Dependency Graph

A dependency graph can be generated anytime by running "make depgraph", which produces a file called "depgraph.svg" in
this directory, which can be viewed by a web browser, and is linked to by this document.

link:depgraph.svg[Dependency Graph]

== TODO

* Remove all code that tries to convert json to a struct/array or vice-versa
* Complete convert coverage
* Modify JSON parser
** Move funcs out of lexnumber, move tests out
** Provide line number, character number, and path in error messages
** JSONValue look up a string path of key names and indexes in objects and arrays, as in "addresses[0].city"
* Search for tests involving maps and make sure they user assert.Equal(t, hardcoded map, returned map)
* Use funcs.Ternary and TernaryFunc more often
* Document Makefile
** Ensure 3 dependency graphs: full, iter and down only, iter and up only
* Create a code generator
** Low level fluent API knows Go switches, var blocks, imports, etc
** Mid level fluent API is more general, not specific to Go
** High level API translates an input file into mid level api calls
*** Define data types (eg, DTO objects) where id is a UUID
*** Convert data type fields to/from a map[string]any
*** DAO objects that CRUD DTOs to all supported free databases
*** queries stored in sql files, where some files are generated, some can be handwritten
*** associate handwritten queries with a name
*** handwritten in a separate dir from generated, so that generated can be cleared and regenerated
*** versioned so that a single code base can handle different versions of data structures and persistence
*** DDL generator that can reconcile current database structure with desired structure
*** One to one: parent has child id
*** One to many: child has parent id
*** Many to many: bridge table of unique (parent id, child id) rows
*** Surrogate key called relid that uses auto generated values from a sequence
*** Option for change columns (last_changed timestamp, last_changed_by string)
*** Full text support
*** Some auto trigger support (eg, deletes cause an insert to a delete tracking table, insert/update puts full text in a separate full text table)
*** Generate new columns on the fly, not just during reconcile (eg, users can define a new column to store)
*** Generate HTTP handling, default CRUD = PUT, GET, PUT, DELETE (can specify POST for C)
* Consider providing simplistic string based arbitrary precision that is not fast, but just does basic operations
* ETL operations
** Mainly operating on streams, with 3 basic operation types:
*** Combine streams
*** Split streams
*** Generate streams
** Look at steps Pentaho and Talend provide as a rough guide
